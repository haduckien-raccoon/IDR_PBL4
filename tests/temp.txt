# ----------------- Old simple match_payload (byte + regex only) -----------------
        
#     def match_payload(self, payload: bytes, meta: Dict[str, Any]):
#         p = payload[: self.payload_bytes]
#         variants = generate_decodes(p, self.enable_decode)
#         hits: List[Tuple[str,str,str]] = []

#         if self.aho:
#             try:
#                 s_raw = p.decode('latin1', errors='ignore')
#                 for end_index, (idx, rid, message) in self.aho.iter(s_raw):
#                     hits.append((rid, message, "AHO_raw"))
#             except Exception:
#                 console_logger.debug("AHO error", exc_info=True)

#         for entry in self.compiled:
#             r = entry["rule"]
#             rule_proto = (r.get("proto") or "ANY").upper()
#             if rule_proto != "ANY" and str(meta.get("proto") or "").upper() != rule_proto:
#                 continue

#             dst_port_rule = r.get("dst_port")
#             dst_port_meta = meta.get("dport")
#             if dst_port_rule is not None and dst_port_meta is not None and dst_port_rule != dst_port_meta:
#                 continue

#             src_port_rule = r.get("src_port")
#             src_port_meta = meta.get("sport")
#             if src_port_rule is not None and src_port_meta is not None and src_port_rule != src_port_meta:
#                 continue

#             pb = entry.get("pattern_bytes")
#             if pb and pb in p:
#                 hits.append((rule_id(r), r.get("message"), "BYTES_raw"))
#                 continue

#             regex = entry.get("pattern_regex_compiled")
#             if regex:
#                 for label, txt in variants:
#                     if regex.search(txt):
#                         hits.append((rule_id(r), r.get("message"), f"REGEX_{label}"))
#                         break

#         # Nếu match rule, log vào ALERTS, không log vào TRAFFIC
#         if hits:
#             for rid, message, variant in hits:
#                 h = hashlib.sha1(f"{rid}|{meta.get('src')}|{meta.get('dst')}|{variant}|{len(p)}".encode()).hexdigest()[:12]
#                 if self.should_throttle(h):
#                     console_logger.debug("throttled alert %s", h)
#                     continue
#                 try:
#                     #lấy thêm action trong rules chứa alerts để biết mức độ nghiêm trọng của alert
#                     if rid in self.rules:
#                         meta["action"] = self.rules[rid].get("action", "unknown")
#                         meta["severity"] = self.rules[rid].get("severity", "medium")
#                     else:
#                         meta["action"] = "unknown"
#                         meta["severity"] = "medium"
#                     action = meta["action"]
#                     severity = meta["severity"]
#                     self.log_alert(meta, p, rid, message, variant, action, severity)
# #fix:
#                     if action.lower() == "block" and str(meta.get("src")) != "127.0.0.1":
#                         src_ip = meta.get("src")
#                         if src_ip:
#                             try:
#                                 enqueue_block(src_ip, reason=f"IDS rule {rid} triggered block action")
#                                 console_logger.info("Enqueued block for %s", src_ip)
#                             except Exception:
#                                 console_logger.exception("enqueue_block error")
#                 except Exception:
#                     console_logger.exception("log_alert error")
#         else:
#             # Nếu không match rule, mới log traffic
#             try:
#                 self.log_traffic(meta, payload)
#             except Exception:
#                 console_logger.exception("log_traffic error")



    # def match_payload(self, payload: bytes, meta: Dict[str, Any]):
    #     """
    #     Full Snort-like payload matching pipeline
    #     1. Build multi-buffer from HTTP fields (uses HTTPParseResult.regions)
    #     2. Fast pattern Aho-Corasick (latin1 mapping) — supports two build styles:
    #     - self.aho is dict[str, Automaton] (per-buffer automata)
    #     - self.aho is Automaton and self.aho_map maps key->list[(rule_idx, content_idx)]
    #     3. Full content match with offset/depth/distance/within
    #     4. PCRE match on decoded variants if required
    #     5. Alert logging + optional block
    #     """

    #     # --- 0. truncate payload if needed ---
    #     p = payload[: self.payload_bytes]

    #     # --- 1. build buffers (multi-buffer like Snort) ---
    #     buffers: Dict[str, bytes] = {"raw": p}

    #     try:
    #         parsed: HTTPParseResult = self.http_parser.parse(p, client_side=True)

    #         # Merge regions from parser (Snort-style). parsed.regions is Dict[str, bytes].
    #         # Only accept bytes values (type-safe).
    #         for region_name, region_value in parsed.regions.items():
    #             if isinstance(region_value, (bytes, bytearray)):
    #                 buffers[region_name] = bytes(region_value)

    #         # NOTE:
    #         # --- DO NOT call undefined some_mapping.update(...) here ---
    #         # We don't need to update any external mapping at this point.
    #         # Buffers are ready and type-safe: Dict[str, bytes].
    #     except Exception:
    #         console_logger.debug("HTTP parse error", exc_info=True)

    #     # in ra http đã parsed (nếu có)
    #     for region_name, region_bytes in buffers.items():
    #         size = len(region_bytes)
    #         snippet = region_bytes[:50]  # hiển thị tối đa 50 byte đầu tiên
    #         try:
    #             snippet_str = snippet.decode("latin1", "ignore")
    #         except Exception:
    #             snippet_str = "<binary data>"
    #         console_logger.info(
    #             "Parsed HTTP region: %-20s | size: %-6d | snippet: %s",
    #             region_name, size, snippet_str
    #         )

    #     # --- prepare hits list ---
    #     hits: List[Tuple[str, str, str]] = []

    #     # --- 2. Fast pattern Aho-Corasick (bytes -> latin1) ---
    #     if self.aho:
    #         try:
    #             # Case A: self.aho is a dict of automata per region (e.g., {"raw": automaton, "http_uri": automaton, ...})
    #             if isinstance(self.aho, dict):
    #                 for buf_name, buf_bytes in buffers.items():
    #                     aho_automaton = self.aho.get(buf_name)
    #                     if not aho_automaton:
    #                         continue
    #                     s = buf_bytes.decode("latin1", "ignore")
    #                     for end_index, val in aho_automaton.iter(s):
    #                         # if automaton stores value as (rule_idx, content_idx) directly adjust accordingly
    #                         # we assume val is the pattern key or stored payload; map to rule indices if needed
    #                         # Best-effort: accept either (rule_idx, content_idx) or key -> lookup in aho_map
    #                         if isinstance(val, tuple) and len(val) == 2 and isinstance(val[0], int):
    #                             rule_idx, content_idx = val
    #                             r = self.compiled[rule_idx]["rule"]
    #                             hits.append((rule_id(r), r.get("message"), f"AHO_{buf_name}"))
    #                         else:
    #                             # val is likely the key string -> use self.aho_map if present
    #                             key = val
    #                             if getattr(self, "aho_map", None):
    #                                 for rule_idx, _ in self.aho_map.get(key, []):
    #                                     r = self.compiled[rule_idx]["rule"]
    #                                     hits.append((rule_id(r), r.get("message"), f"AHO_{buf_name}"))
    #                             else:
    #                                 # fallback: can't map -> still record the key match as generic
    #                                 hits.append((f"AHO_KEY:{key}", f"AHO matched key {key}", f"AHO_{buf_name}"))
    #             else:
    #                 # Case B: self.aho is a single Automaton and self.aho_map maps key->list[(rule_idx, content_idx)]
    #                 automaton = self.aho
    #                 aho_map = getattr(self, "aho_map", {}) or {}
    #                 for buf_name, buf_bytes in buffers.items():
    #                     s = buf_bytes.decode("latin1", "ignore")
    #                     for end_index, val in automaton.iter(s):
    #                         # val is the key string (that's how build_aho added words)
    #                         key = val
    #                         for rule_idx, _ in aho_map.get(key, []):
    #                             r = self.compiled[rule_idx]["rule"]
    #                             hits.append((rule_id(r), r.get("message"), f"AHO_{buf_name}"))
    #         except Exception:
    #             console_logger.debug("AHO error", exc_info=True)
    #     # --- 3. Full content + PCRE match ---
    #     for entry in self.compiled:
    #         r = entry["rule"]

    #         # --- 3a. early proto/port filtering ---
    #         rule_proto = (r.get("proto") or "ANY").upper()
    #         if rule_proto != "ANY" and str(meta.get("proto") or "").upper() != rule_proto:
    #             continue

    #         if r.get("dst_port") is not None and meta.get("dport") is not None:
    #             if r["dst_port"] != meta["dport"]:
    #                 continue
    #         if r.get("src_port") is not None and meta.get("sport") is not None:
    #             if r["src_port"] != meta["sport"]:
    #                 continue

    #         # --- 3b. Full content match ---
    #         contents = entry.get("contents", [])
    #         last_end = 0
    #         rule_matched = True

    #         for idx, c in enumerate(contents):

    #             # correct multi-buffer behavior:
    #             # RULE FIELD *MUST* EXIST — no fallback to "raw"
    #             buf_name = c["raw"].get("field", "raw")

    #             if buf_name not in buffers:
    #                 # Snort/Suricata: nếu FIELD không có → rule FAIL
    #                 rule_matched = False
    #                 break

    #             buf = buffers[buf_name]

    #             # prepare data
    #             pat = c["pattern_bytes"]
    #             nocase = c["raw"].get("nocase", False)
    #             hs = buf.lower() if nocase else buf
    #             needle = pat.lower() if nocase else pat

    #             # --- content[0]: offset + depth ---
    #             if idx == 0:
    #                 start = int(c["raw"].get("offset", 0) or 0)
    #                 depth = c["raw"].get("depth", None)
    #                 end = start + depth if (depth is not None and depth >= 0) else len(buf)

    #             # --- content[1+]: distance + within ---
    #             else:
    #                 distance = int(c["raw"].get("distance", 0) or 0)
    #                 start = last_end + distance
    #                 within = c["raw"].get("within", None)
    #                 end = last_end + within if (within is not None and within >= 0) else len(buf)

    #             # bounds
    #             start = max(0, start)
    #             end = min(len(buf), end)
    #             if start > end:
    #                 rule_matched = False
    #                 break

    #             # match
    #             pos = hs.find(needle, start, end)
    #             if pos < 0:
    #                 rule_matched = False
    #                 break

    #             last_end = pos + len(needle)

    #         if not rule_matched:
    #             continue

    #         # --- 3c. PCRE match ---
    #         pcre = entry.get("pcre_compiled")
    #         if pcre:
    #             matched_pcre = False
    #             variants = generate_decodes(buf, enable_decode=self.enable_decode)

    #             for label, txt in variants:
    #                 if isinstance(txt, bytes):
    #                     txt = txt.decode("latin1", "ignore")
    #                 try:
    #                     if pcre.search(txt):
    #                         matched_pcre = True
    #                         break
    #                 except Exception:
    #                     pass

    #             if not matched_pcre:
    #                 continue

    #         # --- 3d. MATCHED RULE ---
    #         hits.append((rule_id(r), r.get("message"), f"FULL_{buf_name}"))


    #     # --- 4. Log alerts or traffic ---
    #     if hits:
    #         for rid, message, variant in hits:
    #             h = hashlib.sha1(f"{rid}|{meta.get('src')}|{meta.get('dst')}|{variant}|{len(p)}".encode()).hexdigest()[:12]
    #             if self.should_throttle(h):
    #                 console_logger.debug("throttled alert %s", h)
    #                 continue
    #             try:
    #                 # add action + severity
    #                 if rid in self.rules:
    #                     meta["action"] = self.rules[rid].get("action", "unknown")
    #                     meta["severity"] = self.rules[rid].get("severity", "medium")
    #                 else:
    #                     meta["action"] = "unknown"
    #                     meta["severity"] = "medium"
    #                 action = meta["action"]
    #                 severity = meta["severity"]
    #                 self.log_alert(meta, p, rid, message, variant, action, severity)

    #                 # Block if requested
    #                 # if action.lower() == "block" and str(meta.get("src")) != "127.0.0.1":
    #                 #     src_ip = meta.get("src")
    #                 #     if src_ip:
    #                 #         try:
    #                 #             enqueue_block(src_ip, reason=f"IDS rule {rid} triggered block action")
    #                 #             console_logger.info("Enqueued block for %s", src_ip)
    #                 #         except Exception:
    #                 #             console_logger.exception("enqueue_block error")
    #             except Exception:
    #                 console_logger.exception("log_alert error")
    #     else:
    #         try:
    #             self.log_traffic(meta, payload)
    #         except Exception:
    #             console_logger.exception("log_traffic error")

    # #RFC 793 + RFC 815 = suritaca
from __future__ import annotations
import sys
import os
ROOT_DIR = os.path.abspath(os.path.join(os.path.dirname(__file__), "..", ".."))
if ROOT_DIR not in sys.path:
    sys.path.insert(0, ROOT_DIR)
import argparse
import threading
import queue
import time
import logging
import re
import binascii
import math
import base64
from collections import Counter
from scapy.all import sniff, IP, TCP, UDP, Raw
from pathlib import Path
import json
import hashlib
from urllib.parse import unquote_plus
from typing import Dict, Any, Tuple, List, Optional
from watchdog.observers import Observer
from watchdog.events import FileSystemEventHandler
import requests
from typing import Deque, Tuple, Dict, Any, List, Optional
from collections import deque
import sys
import threading
from app.capture_packet.flowtracker_module import FlowTracker

# console_logger = logging.getLogger("console")
# BUFID = Tuple[str, str, int, int]  # (src_ip, dst_ip, src_port, dst_port)

# class TCPReassembly:
#     def __init__(self):
#         # buffer[BUFID] = {
#         #   'hdl': [ {'first': int_seq, 'last': int_seq}, ... ],
#         #   ack_num (int): {
#         #       'ind': [indexes],
#         #       'isn': int,    # initial sequence number for this ack-buffer
#         #       'len': int,    # current length of raw
#         #       'raw': bytearray,
#         #   }, ...
#         # }
#         self._buffer: Dict[BUFID, Dict] = {}
#         # produced datagrams (list of dict)
#         self._datagrams: List[Dict[str, Any]] = []
#         #Flow tracker can be added here if needed for advanced state tracking
#         self.flow_tracker = FlowTracker()
#         #------------------------------#
#         # NEW: memory / timeout controls
#         self.memcap = 200 * 1024 * 1024  # 200MB total reassembly memcap (tuneable)
#         self.idle_timeout = 120          # seconds to keep idle flows (tuneable)
#         self.max_ack_blocks_per_flow = 50
#         self.max_holes_per_flow = 64
#         self.max_flow_raw_bytes = 2 * 1024 * 1024  # 2MB per ack-block (tuneable)
#         #------------------------------#

#     #--------------------New--------------------------#
#     def memory_usage(self) -> int:
#         """Compute approximate memory used by raw buffers in _buffer."""
#         total = 0
#         for buf in self._buffer.values():
#             for k, v in buf.items():
#                 if isinstance(k, int) and isinstance(v, dict):
#                     raw = v.get('raw')
#                     if raw:
#                         total += len(raw)
#         return total

#     def cleanup_timeouts(self):
#         """Remove idle flows or enforce memcap."""
#         now = time.time()
#         # 1) prune idle flows
#         to_del = []
#         for bufid, buf in list(self._buffer.items()):
#             last = buf.get('last_seen')
#             if last is None:
#                 continue
#             if now - last > self.idle_timeout:
#                 to_del.append((bufid, 'idle_timeout'))

#         for bufid, reason in to_del:
#             try:
#                 self._submit_and_delete(bufid, reason=reason)
#             except Exception:
#                 # defensive: ensure deletion
#                 self._buffer.pop(bufid, None)

#         # 2) enforce memcap (flush oldest until under cap)
#         mem = self.memory_usage()
#         if mem <= self.memcap:
#             return
#         # sort buffers by last_seen ascending (oldest first) and flush until mem below cap
#         items = sorted(
#             ((bufid, buf.get('last_seen', 0)) for bufid, buf in self._buffer.items()),
#             key=lambda x: x[1] or 0
#         )
#         for bufid, _ in items:
#             if mem <= self.memcap:
#                 break
#             try:
#                 self._submit_and_delete(bufid, reason='memcap_flush')
#                 # recompute mem delta conservatively
#                 mem = self.memory_usage()
#             except Exception:
#                 # ensure removal if submit fails
#                 self._buffer.pop(bufid, None)
#                 mem = self.memory_usage()
#         #------------------------------#

#     # ---------- public helpers ----------
#     def process_packet(self, pkt, index: Optional[int] = None):
#         """Process a Scapy packet for reassembly. Call this for each captured packet."""
#         if not pkt.haslayer(IP) or not pkt.haslayer(TCP):
#             return

#         ip = pkt[IP]
#         tcp = pkt[TCP]
#         # if tcp.dport != 80:
#         #     return
        
#         payload = bytes(tcp.payload)
#         has_payload = len(payload) > 0

#         BUFID = (str(ip.src), str(ip.dst), int(tcp.sport), int(tcp.dport))
#         self.flow_tracker.update(BUFID, tcp, ip)
#         DSN = int(tcp.seq)
#         ACK = int(tcp.ack)
#         SYN = bool(tcp.flags & 0x02)
#         FIN = bool(tcp.flags & 0x01)
#         RST = bool(tcp.flags & 0x04)
#         FIRST = DSN
#         LAST = DSN + len(payload)

#         try: 
#             self.flow_tracker.update(BUFID, tcp, ip)
#         except Exception:
#             pass

#         # If SYN and an existing buffer exists => flush previous and delete
#         if SYN and BUFID in self._buffer:
#             self._submit_and_delete(BUFID, reason='syn_reset')

#         # If buffer not exist, create new one
#         if BUFID not in self._buffer:
#             # Create HDL such that after first received fragment, missing region starts at DSN+len(payload)
#             # We initialize HDL as wide open; we'll update after inserting fragment
#             self._buffer[BUFID] = {
#                 'hdl': [],  # will set when first fragment arrives
#             }
#             #--------------------#
#             self._buffer[BUFID]['last_seen'] = time.time() #New : timestamp of last activity
#             #--------------------#

#         # If no payload, still record ACK entry (to keep indices) and flush on FIN/RST if present
#         if not has_payload:
#             # ensure an ACK entry exists
#             if ACK not in self._buffer[BUFID]:
#                 self._buffer[BUFID][ACK] = {
#                     'ind': [index] if index is not None else [],
#                     'isn': DSN,
#                     'len': 0,
#                     'raw': bytearray(),
#                 }
#             else:
#                 if index is not None:
#                     self._buffer[BUFID][ACK]['ind'].append(index)
#             if FIN or RST:
#                 self._submit_and_delete(BUFID, reason='fin_or_rst_no_payload')
#             return

#         # Insert payload into ACK-specific block
#         if ACK not in self._buffer[BUFID]:
#             # create new block; set ISN to DSN and raw to payload
#             self._buffer[BUFID][ACK] = {
#                 'ind': [index] if index is not None else [],
#                 'isn': DSN,
#                 'len': len(payload),
#                 'raw': bytearray(payload),
#             }
#             # If we just created first block in this session, initialize HDL to indicate missing after this fragment
#             if not self._buffer[BUFID].get('hdl'):
#                 # Hole starts at LAST (next wanted seq) and is unbounded to the right
#                 self._buffer[BUFID]['hdl'] = [{'first': LAST, 'last': sys.maxsize}]
#         else:
#             # append index
#             if index is not None:
#                 self._buffer[BUFID][ACK]['ind'].append(index)

#             # merge fragment into existing raw
#             block = self._buffer[BUFID][ACK]
#             ISN = block['isn']
#             RAW = block['raw']

#             if DSN >= ISN:
#                 # fragment starts at or after ISN
#                 offset = DSN - ISN
#                 needed = offset + len(payload)
#                 if offset >= len(RAW):
#                     # append gap (zeros) then payload
#                     gap = offset - len(RAW)
#                     if gap > 0:
#                         RAW.extend(b'\x00' * gap)
#                     RAW.extend(payload)
#                 else:
#                     # overlapping or replacing bytes
#                     endpos = offset + len(payload)
#                     if endpos > len(RAW):
#                         # extend to fit
#                         RAW[offset:endpos] = payload
#                     else:
#                         RAW[offset:endpos] = payload
#             else:
#                 # fragment starts before ISN -> need to prepend or overlap-left
#                 # compute overlap / gap relative to ISN
#                 delta = ISN - DSN  # bytes that fragment extends left of ISN
#                 if delta >= len(payload):
#                     # fragment entirely before current RAW with gap
#                     gap = delta - len(payload)
#                     RAW = bytearray(payload + (b'\x00' * gap) + RAW)
#                     block['isn'] = DSN
#                 else:
#                     # partial overlap: prefix from payload that is before ISN, then remainder overlaps existing RAW
#                     prefix = payload[:delta]
#                     overlap = payload[delta:]
#                     RAW = bytearray(prefix + RAW)
#                     # now write overlap into RAW starting at position len(prefix)
#                     pos = len(prefix)
#                     need = pos + len(overlap)
#                     if need > len(RAW):
#                         RAW.extend(b'\x00' * (need - len(RAW)))
#                     RAW[pos:pos + len(overlap)] = overlap
#                     block['isn'] = DSN
#             block['raw'] = RAW
#             block['len'] = len(block['raw'])
#             #New -----------------------#
#             # NEW: enforce per-block raw size limit to prevent memory blowup
#             if block['len'] > self.max_flow_raw_bytes:
#                 # too big -> flush whole buffer
#                 self._submit_and_delete(BUFID, reason='raw_size_exceeded')
#                 return
#             #------------------------------#
            
#         # Update HDL using RFC-815 like logic: holes described in absolute seq numbers
#         HDL = self._buffer[BUFID].get('hdl', [])
#         # If HDL empty, we can set a fresh hole starting after this block (LAST) if not set
#         if not HDL:
#             HDL = [{'first': LAST, 'last': sys.maxsize}]
#             self._buffer[BUFID]['hdl'] = HDL

#         # Find a hole that overlaps with [FIRST, LAST)
#         for idx, hole in enumerate(list(HDL)):
#             # If fragment entirely after this hole -> continue
#             if FIRST > hole['last']:
#                 continue
#             # If fragment entirely before this hole -> continue
#             if LAST < hole['first']:
#                 continue
#             # Overlap: remove current hole
#             try:
#                 HDL.pop(idx)
#             except Exception:
#                 # safe fallback: rebuild without this hole
#                 HDL = [h for h in HDL if h is not hole]
#             # left leftover
#             if FIRST > hole['first']:
#                 left = {'first': hole['first'], 'last': FIRST - 1}
#                 HDL.insert(idx, left)
#                 idx += 1
#             # right leftover (only create if fragment does not finalize and not FIN/RST)
#             if (LAST < hole['last']) and (not FIN) and (not RST):
#                 right = {'first': LAST + 1, 'last': hole['last']}
#                 HDL.insert(idx, right)
#             break
#         # store HDL back
#         self._buffer[BUFID]['hdl'] = HDL
#         #------------------------------#
#         # NEW: update last activity timestamp after processing fragment
#         self._buffer[BUFID]['last_seen'] = time.time()

#         # NEW: defensive limits: avoid unbounded growth per-flow
#         # limit number of ACK blocks kept per flow
#         ack_blocks = [k for k in self._buffer[BUFID].keys() if isinstance(k, int)]
#         if len(ack_blocks) > 50:  # <-- tuneable threshold
#             # too many ACK blocks -> flush oldest to free memory
#             # choose oldest by their 'isn' or just flush this buffer
#             self._submit_and_delete(BUFID, reason='too_many_ack_blocks')
#             return

#         # limit hole-list growth
#         if len(HDL) > 64:  # <-- tuneable threshold
#             # compress to single open hole (keeps algorithm but prevents explosion)
#             self._buffer[BUFID]['hdl'] = [{'first': HDL[0]['first'] if HDL else LAST, 'last': sys.maxsize}]
#             # update last_seen again
#             self._buffer[BUFID]['last_seen'] = time.time()
#         #------------------------------#    

#         # If FIN or RST present, flush session
#         if FIN or RST:
#             self._submit_and_delete(BUFID, reason='fin_or_rst')

#     def _submit_and_delete(self, bufid: BUFID, reason: str = 'flush'):
#         """Build datagrams from buffer[bufid] and remove the buffer."""
#         if bufid not in self._buffer:
#             return
#         buf = self._buffer[bufid]
#         HDL = buf.get('hdl', [])
#         # fetch all fragments (acks) and produce datagrams
#         # pass pkt tuple None for now; can be extended if needed
#         flow_info = self.flow_tracker.get_flow_safe(bufid, pkt=None)

#         # iterate all ack-keys in buf (ints)
#         for key, block in list(buf.items()):
#             if key == 'hdl':
#                 continue
#             if not isinstance(key, int):
#                 continue
#             raw = block.get('raw', None)
#             if not raw:
#                 continue
#             payload_bytes = bytes(raw)
#             datagram = {
#                 'flow': flow_info,
#                 'NotImplemented': (len(HDL) != 0),  # True if holes remain
#                 'id': {
#                     'src': (bufid[0], bufid[2]),
#                     'dst': (bufid[1], bufid[3]),
#                     'ack': key,
#                 },
#                 'index': tuple(block.get('ind', [])),
#                 'payload': payload_bytes,
#                 'packets': None,
#                 'flush_reason': reason,
#             }
            
#             self._datagrams.append(datagram)
#         # finally delete buffer
#         try:
#             del self._buffer[bufid]
#         except KeyError:
#             pass
        
#         #Flow cleanup 
#         try:
#             f = self.flow_tracker.get_flow(bufid)
#             #get_flow tra none neu khong co flow
#             if f and f.get('state') == 'teardown':
#                 self.flow_tracker.delete_flow(bufid)
#         except Exception:
#             pass

#     def get_datagrams(self) -> List[Dict[str, Any]]:
#         """Return list of produced datagrams (and keep them)."""
#         return list(self._datagrams)

#     def clear_datagrams(self):
#         """Clear stored datagrams."""
#         self._datagrams.clear()

#     def flush_all(self):
#         """Flush all active buffers (force produce datagrams) and clear buffers."""
#         bufids = list(self._buffer.keys())
#         for b in bufids:
#             self._submit_and_delete(b, reason='manual_flush')

#     def get_buffer_snapshot(self) -> Dict:
#         """Debug helper: snapshot of current buffers and HDL."""
#         snap = {}
#         for k, v in self._buffer.items():
#             snap[k] = {
#                 'hdl': v.get('hdl'),
#                 'acks': [x for x in v.keys() if isinstance(x, int)],
#             }
#         return snap

# # ---------- Wrapper to expose feed(ip_pkt) API ----------
# class TCPReassembler:
#     """
#     Wrapper around TCPReassembly that exposes `feed(ip_pkt)` returning either
#     (assembled_bytes, (src, dst, sport, dport)) or None.
#     It buffers multiple produced datagrams internally and returns one per call.
#     """
#     def __init__(self, timeout: int = 120):
#         self.reasm = TCPReassembly()
#         self.lock = threading.Lock()
#         self._outq: Deque[Tuple[bytes, Tuple[str,str,int,int]]] = deque()
#         # optional timeout attribute kept for compatibility with old class
#         self.timeout = timeout

#     def feed(self, ip_pkt) -> Optional[Tuple[bytes, Tuple[str,str,int,int]]]:
#         # NEW: periodic cleanup to avoid leak #
#         try:
#             # run lightweight cleanup: prune idle flows & enforce memcap
#             self.reasm.cleanup_timeouts()
#         except Exception:
#             console_logger.debug("cleanup_timeouts error", exc_info=True)

#         # return any queued assembled datagram first
#         with self.lock:
#             if self._outq:
#                 return self._outq.popleft()

#             # process incoming packet via standard reassembly
#             try:
#                 self.reasm.process_packet(ip_pkt)
#             except Exception:
#                 # avoid blowing up worker loop on unexpected pkt shapes
#                 # log if needed, but keep behavior silent here
#                 pass

#             datagrams = self.reasm.get_datagrams()
#             if not datagrams:
#                 return None

#             # push all datagrams into outq (as (payload, key)), then clear
#             for d in datagrams:
#                 payload = d.get('payload', b'')
#                 # datagram id: 'src': (ip, port), 'dst': (ip, port)
#                 idinfo = d.get('id', {})
#                 src = idinfo.get('src', (None, None))
#                 dst = idinfo.get('dst', (None, None))
#                 try:
#                     key = (str(src[0]), str(dst[0]), int(src[1]), int(dst[1]) if dst[1] is not None else None)
#                 except Exception:
#                     # fallback to values from packet if id formatting unexpected
#                     try:
#                         t = ip_pkt[TCP]
#                         key = (str(ip_pkt[IP].src), str(ip_pkt[IP].dst), int(t.sport), int(t.dport))
#                     except Exception:
#                         key = (None, None, None, None)
#                 self._outq.append((payload, key))

#             # clear datagrams stored in TCPReassembly to avoid duplication
#             self.reasm.clear_datagrams()

#             if self._outq:
#                 return self._outq.popleft()
#             return None
        

#     def _cleanup(self):
#         # kept for API parity; reassembly uses internal cleanup via sys.maxsize holes,
#         # you can implement timed connection culling here if needed.
#         try: 
#             self.reasm.flow_tracker.prune_stale()
#         except Exception:
#             pass
# def _worker_loop_shard(worker_id: int, ids: "IDS", q: "queue.Queue", stop_event: threading.Event, metrics: Dict[str, List[int]]):
#     allowed_ports = {80}  # HTTP
#     console_logger.info("Worker %d started", worker_id)

#     while not stop_event.is_set():
#         try:
#             pkt = q.get(timeout=0.5)
#         except queue.Empty:
#             try:
#                 ids.reasm._cleanup()
#             except Exception:
#                 pass
#             continue

#         try:
#             if IP not in pkt or TCP not in pkt:
#                 continue

#             ip_pkt = pkt[IP]
#             tcp_pkt = ip_pkt[TCP]

#             out = ids.reasm.feed(ip_pkt)
#             if out:
#                 assembled_bytes, conn_key = out
#                 src, dst, sport, dport = conn_key

#                 # --- Xác định direction: request hay response ---
#                 if int(dport) in allowed_ports:
#                     # client -> server (request)
#                     client_side = True
#                     meta = {"src": src, "dst": dst, "sport": int(sport), "dport": int(dport), "proto": "TCP"}
#                 elif int(sport) in allowed_ports:
#                     # server -> client (response)
#                     client_side = False
#                     meta = {"src": src, "dst": dst, "sport": int(sport), "dport": int(dport), "proto": "TCP"}
#                 else:
#                     continue

#                 # --- parse HTTP ---
#                 try:
#                     parsed = ids.http_parser.parse(assembled_bytes, client_side=client_side)
#                     if not client_side:
#                         meta["status_code"] = parsed.status_code
#                 except Exception:
#                     console_logger.debug("HTTP parse error", exc_info=True)

#                 ids.match_payload(assembled_bytes, meta)

#             else:
#                 # xử lý payload lẻ (không reassembled)
#                 raw_payload = bytes(tcp_pkt.payload) if Raw in tcp_pkt else b""
#                 if not raw_payload:
#                     continue

#                 if int(tcp_pkt.dport) in allowed_ports:
#                     client_side = True
#                     meta = {"src": ip_pkt.src, "dst": ip_pkt.dst, "sport": int(tcp_pkt.sport), "dport": int(tcp_pkt.dport), "proto": "TCP"}
#                 elif int(tcp_pkt.sport) in allowed_ports:
#                     client_side = False
#                     meta = {"src": ip_pkt.src, "dst": ip_pkt.dst, "sport": int(tcp_pkt.sport), "dport": int(tcp_pkt.dport), "proto": "TCP"}
#                 else:
#                     continue

#                 try:
#                     parsed = ids.http_parser.parse(raw_payload, client_side=client_side)
#                     if not client_side:
#                         meta["status_code"] = parsed.status_code
#                 except Exception:
#                     console_logger.debug("HTTP parse error", exc_info=True)

#                 ids.match_payload(raw_payload, meta)

#             metrics["proc"][worker_id] += 1

#         except Exception:
#             console_logger.exception("Worker %d loop exception", worker_id)
#         finally:
#             try:
#                 q.task_done()
#             except Exception:
#                 pass
